#!/usr/bin/env python3
"""
ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v2.1 - GitHub Actionsä¼˜åŒ–ç‰ˆ
ä¸“ä¸ºGitHub Actionsç¯å¢ƒä¼˜åŒ–ï¼Œæä¾›æ›´å¥½çš„ç¨³å®šæ€§å’Œé”™è¯¯å¤„ç†
æ”¯æŒ60+ä¿¡æ¯æºï¼Œç”ŸæˆSRTå­—å¹•æ–‡ä»¶
"""

import os
import sys
import time
import json
import feedparser
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import traceback
import re

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from premium_sources import GitHubOptimizedSources
from intelligent_classifier import IntelligentClassifier
from report_generator import ReportGenerator
from notification_system import NotificationSystem

class GitHubOptimizedAggregator:
    """GitHub Actionsä¼˜åŒ–ç‰ˆç§‘æŠ€èµ„è®¯èšåˆå™¨ v2.1"""
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–GitHub Actionsä¼˜åŒ–ç‰ˆèšåˆå™¨ v2.1...")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.sources_manager = GitHubOptimizedSources()
        self.classifier = IntelligentClassifier()
        self.report_generator = ReportGenerator()
        self.notification_system = NotificationSystem()
        
        # é…ç½®å‚æ•°
        self.max_retries = 3
        self.retry_delay = 2  # ç§’
        self.timeout = 30  # ç§’
        self.min_articles_threshold = 10  # æœ€å°‘æ–‡ç« æ•°é‡ï¼ˆæé«˜é˜ˆå€¼ï¼‰
        
        # æ‰“å°ä¿¡æ¯æºç»Ÿè®¡
        self.sources_manager.print_sources_summary()
        
        print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def run_daily_collection(self):
        """è¿è¡Œæ¯æ—¥æ”¶é›†ä»»åŠ¡ - GitHub Actionsä¼˜åŒ–ç‰ˆ v2.1"""
        print("\nğŸš€ å¼€å§‹æ¯æ—¥ç§‘æŠ€èµ„è®¯æ”¶é›† (v2.1)")
        print("ğŸ“Š ä¿¡æ¯æºå·²æ‰©å±•è‡³60+ä¸ªä¼˜è´¨ç§‘æŠ€åª’ä½“")
        print("ğŸ¬ æ–°å¢SRTå­—å¹•æ–‡ä»¶ç”ŸæˆåŠŸèƒ½")
        print("ğŸ¯ ä¸“æ³¨åå¤§æ ¸å¿ƒé¢†åŸŸï¼šAIã€GitHubã€åˆ›ä¸šã€ç¡…è°·ã€ç§‘æŠ€ã€å¼€æºã€ä¼ä¸šã€ç§»åŠ¨ã€å®‰å…¨ã€Webå¼€å‘")
        print("ğŸ›¡ï¸ å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("=" * 80)
        
        try:
            # 1. æ”¶é›†RSSæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            print("ğŸ“¡ æ­£åœ¨æ”¶é›†RSSæ•°æ®...")
            news_data = self._collect_rss_data_with_retry()
            print(f"âœ… æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(news_data)} æ¡èµ„è®¯")
            
            # æ£€æŸ¥æœ€å°‘æ–‡ç« æ•°é‡
            if len(news_data) < self.min_articles_threshold:
                print(f"âš ï¸ æ–‡ç« æ•°é‡è¿‡å°‘ ({len(news_data)} < {self.min_articles_threshold})")
                print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨ç­–ç•¥...")
                news_data = self._fallback_collection_strategy()
            
            # 2. æ™ºèƒ½åˆ†ç±»å’Œå¤„ç†
            print("ğŸ§  æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†ç±»å’Œå¤„ç†...")
            classified_data = self._classify_and_process(news_data)
            print(f"âœ… åˆ†ç±»å®Œæˆï¼Œä¿ç•™ {len(classified_data)} æ¡é«˜è´¨é‡èµ„è®¯")
            
            # 3. ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡
            self._print_classification_stats(classified_data)
            
            # 4. ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬çš„æŠ¥å‘Šï¼ˆåŒ…å«SRTå­—å¹•ï¼‰
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆå¤šç‰ˆæœ¬æŠ¥å‘Š...")
            versions = self.report_generator.generate_all_versions(classified_data)
            print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            # æ‰“å°ç‰ˆæœ¬ä¿¡æ¯
            self._print_versions_info(versions)
            
            # 5. å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            print("ğŸ“§ æ­£åœ¨å‘é€é‚®ä»¶é€šçŸ¥...")
            try:
                success = self.notification_system.send_daily_reports(versions)
                if success:
                    print("âœ… é‚®ä»¶å‘é€æˆåŠŸ")
                else:
                    print("âš ï¸ é‚®ä»¶å‘é€å¤±è´¥ï¼Œä½†ç³»ç»Ÿç»§ç»­è¿è¡Œ")
            except Exception as e:
                print(f"âš ï¸ é‚®ä»¶å‘é€å¼‚å¸¸: {e}")
                print("ğŸ“§ é‚®ä»¶åŠŸèƒ½å¯èƒ½éœ€è¦é…ç½®ç¯å¢ƒå˜é‡")
            
            # 6. ä¿å­˜åˆ°GitHub
            self._save_for_github(versions, classified_data)
            
            print("ğŸ‰ æ¯æ—¥æ”¶é›†ä»»åŠ¡å®Œæˆï¼")
            print("ğŸ’¡ åŸºäºGitHub Actionsä¼˜åŒ–çš„é«˜è´¨é‡ä¿¡æ¯æº")
            print("ğŸ¬ SRTå­—å¹•æ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯ç›´æ¥ç”¨äºæ’­å®¢åˆ¶ä½œ")
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            traceback.print_exc()
            
            # å°è¯•ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            self._generate_error_report(str(e))
            sys.exit(1)
    
    def _collect_rss_data_with_retry(self) -> List[Dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„RSSæ•°æ®æ”¶é›†"""
        all_news = []
        sources = self.sources_manager.get_all_sources()
        
        # æŒ‰å¯é æ€§æ’åºï¼Œä¼˜å…ˆå¤„ç†é«˜å¯é æ€§æº
        sources.sort(key=lambda x: (
            x.get('reliability') == 'excellent',
            x.get('priority') == 'high'
        ), reverse=True)
        
        successful_sources = 0
        failed_sources = []
        
        for source in sources:
            print(f"ğŸ“¡ å¤„ç†: {source['name']}...", end=' ')
            
            success = False
            for attempt in range(self.max_retries):
                try:
                    # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (compatible; TechNewsBot/2.1; +https://github.com/tech-news-bot)'
                    }
                    
                    # ä½¿ç”¨requestsè·å–RSSå†…å®¹
                    response = requests.get(
                        source['url'], 
                        headers=headers, 
                        timeout=self.timeout
                    )
                    response.raise_for_status()
                    
                    # è§£æRSS
                    feed = feedparser.parse(response.content)
                    
                    if not feed.entries:
                        print(f"âŒ æ— å†…å®¹ (å°è¯• {attempt + 1}/{self.max_retries})")
                        if attempt < self.max_retries - 1:
                            time.sleep(self.retry_delay)
                        continue
                    
                    # å¤„ç†RSSæ¡ç›®
                    recent_count = 0
                    for entry in feed.entries[:20]:  # é™åˆ¶å¤„ç†æ•°é‡
                        try:
                            news_item = self._parse_rss_entry(entry, source)
                            if news_item and self._is_recent_news(news_item.get('time')):
                                all_news.append(news_item)
                                recent_count += 1
                        except Exception as parse_error:
                            print(f"è§£ææ¡ç›®é”™è¯¯: {parse_error}")
                            continue
                    
                    print(f"âœ… {recent_count}æ¡")
                    successful_sources += 1
                    success = True
                    break
                    
                except requests.exceptions.Timeout:
                    print(f"â° è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                except requests.exceptions.RequestException as e:
                    print(f"ğŸŒ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:30]}...")
                except Exception as e:
                    print(f"âŒ é”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:30]}...")
                
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
            
            if not success:
                failed_sources.append(source['name'])
        
        print(f"\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸæº: {successful_sources}/{len(sources)}")
        print(f"   âŒ å¤±è´¥æº: {len(failed_sources)}")
        if failed_sources:
            print(f"   å¤±è´¥åˆ—è¡¨: {', '.join(failed_sources[:5])}{'...' if len(failed_sources) > 5 else ''}")
        
        return all_news
    
    def _fallback_collection_strategy(self) -> List[Dict]:
        """å¤‡ç”¨æ”¶é›†ç­–ç•¥"""
        print("ğŸ”„ æ‰§è¡Œå¤‡ç”¨æ”¶é›†ç­–ç•¥...")
        
        # 1. æ‰©å±•æ—¶é—´çª—å£åˆ°48å°æ—¶
        print("   ğŸ“… æ‰©å±•æ—¶é—´çª—å£åˆ°48å°æ—¶")
        all_news = []
        
        # åªä½¿ç”¨æœ€å¯é çš„æº
        reliable_sources = self.sources_manager.get_excellent_reliability_sources()
        
        for source in reliable_sources[:10]:  # é™åˆ¶æ•°é‡
            try:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries[:30]:  # å¢åŠ å¤„ç†æ•°é‡
                    try:
                        news_item = self._parse_rss_entry(entry, source)
                        if news_item:
                            # æ‰©å±•æ—¶é—´çª—å£
                            cutoff_time = datetime.now() - timedelta(hours=48)
                            if news_item.get('time') and news_item['time'] > cutoff_time:
                                all_news.append(news_item)
                    except:
                        continue
            except:
                continue
        
        print(f"   ğŸ“° å¤‡ç”¨ç­–ç•¥æ”¶é›†åˆ° {len(all_news)} æ¡èµ„è®¯")
        return all_news
    
    def _parse_rss_entry(self, entry, source: Dict) -> Optional[Dict]:
        """è§£æRSSæ¡ç›®"""
        try:
            # è·å–åŸºæœ¬ä¿¡æ¯
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            summary = entry.get('summary', entry.get('description', '')).strip()
            link = entry.get('link', '')
            
            # è§£ææ—¶é—´
            pub_time = self._parse_time(entry.get('published', ''))
            
            return {
                'title': title,
                'summary': summary[:500],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                'url': link,
                'source': source['name'],
                'category': source['category'],
                'time': pub_time,
                'priority': source.get('priority', 'medium'),
                'reliability': source.get('reliability', 'good')
            }
        except Exception as e:
            print(f"è§£æRSSæ¡ç›®é”™è¯¯: {e}")
            return None
    
    def _parse_time(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            import dateutil.parser
            parsed_time = dateutil.parser.parse(time_str)
            if parsed_time.tzinfo is not None:
                parsed_time = parsed_time.replace(tzinfo=None)
            return parsed_time
        except:
            return datetime.now()
    
    def _is_recent_news(self, news_time) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ€è¿‘çš„æ–°é—»"""
        if not isinstance(news_time, datetime):
            return True
        
        cutoff_time = datetime.now() - timedelta(hours=24)
        return news_time > cutoff_time
    
    def _classify_and_process(self, news_data: List[Dict]) -> List[Dict]:
        """åˆ†ç±»å’Œå¤„ç†æ–°é—»æ•°æ®"""
        if not news_data:
            return []
        
        # å»é‡
        unique_news = self._remove_duplicates(news_data)
        print(f"   ğŸ”„ å»é‡å: {len(unique_news)} æ¡")
        
        # æ™ºèƒ½åˆ†ç±»
        classified_news = []
        for news in unique_news:
            try:
                classified = self.classifier.classify_news(news)
                classified_news.append(classified)
            except Exception as e:
                print(f"åˆ†ç±»é”™è¯¯: {e}")
                classified_news.append(news)  # ä¿ç•™åŸå§‹æ•°æ®
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        classified_news.sort(
            key=lambda x: x.get('ç»¼åˆè¯„åˆ†', 0), 
            reverse=True
        )
        
        return classified_news
    
    def _remove_duplicates(self, news_data: List[Dict]) -> List[Dict]:
        """å»é‡å¤„ç†"""
        seen_titles = set()
        unique_news = []
        
        for news in news_data:
            title_key = news.get('title', '').lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news
    
    def _print_classification_stats(self, classified_data: List[Dict]):
        """æ‰“å°åˆ†ç±»ç»Ÿè®¡ - ä¼˜åŒ–ç‰ˆ"""
        if not classified_data:
            print("âš ï¸ æ²¡æœ‰åˆ†ç±»æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†ç±»
        category_stats = {}
        priority_stats = {}
        reliability_stats = {}
        
        for news in classified_data:
            # åˆ†ç±»ç»Ÿè®¡
            main_cat = news.get('ä¸»åˆ†ç±»', news.get('category', 'æœªåˆ†ç±»'))
            category_stats[main_cat] = category_stats.get(main_cat, 0) + 1
            
            # ä¼˜å…ˆçº§ç»Ÿè®¡
            priority = news.get('priority', 'medium')
            priority_stats[priority] = priority_stats.get(priority, 0) + 1
            
            # å¯é æ€§ç»Ÿè®¡
            reliability = news.get('reliability', 'good')
            reliability_stats[reliability] = reliability_stats.get(reliability, 0) + 1
        
        print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        
        # åˆ†ç±»åˆ†å¸ƒ
        print("   ğŸ“‚ åˆ†ç±»åˆ†å¸ƒ:")
        categories = self.sources_manager.get_categories()
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            category_name = categories.get(category, category)
            percentage = count / len(classified_data) * 100
            print(f"      {category_name}: {count} æ¡ ({percentage:.1f}%)")
        
        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        print("   ğŸ”¥ ä¼˜å…ˆçº§åˆ†å¸ƒ:")
        for priority, count in sorted(priority_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(classified_data) * 100
            icon = "ğŸ”¥" if priority == "high" else "ğŸ“Œ" if priority == "medium" else "ğŸ“"
            print(f"      {icon} {priority}: {count} æ¡ ({percentage:.1f}%)")
    
    def _print_versions_info(self, versions: Dict[str, str]):
        """æ‰“å°ç‰ˆæœ¬ä¿¡æ¯"""
        print("ğŸ“‹ ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯:")
        for version_name, content in versions.items():
            char_count = len(content)
            line_count = content.count('\n')
            
            if version_name == 'english':
                print(f"   ğŸ“„ è‹±æ–‡ç‰ˆ: {char_count:,} å­—ç¬¦, {line_count} è¡Œ")
            elif version_name == 'bilingual':
                print(f"   ğŸŒ ä¸­è‹±æ··åˆç‰ˆ: {char_count:,} å­—ç¬¦, {line_count} è¡Œ")
            elif version_name == 'srt':
                # ç»Ÿè®¡SRTå­—å¹•æ¡æ•°
                srt_count = content.count('\n\n') if content else 0
                print(f"   ğŸ¬ SRTå­—å¹•ç‰ˆ: {srt_count} æ¡å­—å¹•, {char_count:,} å­—ç¬¦")
    
    def _save_for_github(self, versions: Dict[str, str], classified_data: List[Dict]):
        """ä¿å­˜æ–‡ä»¶ä¾›GitHub Actionsä½¿ç”¨ - ä¼˜åŒ–ç‰ˆ"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path('output')
        output_dir.mkdir(exist_ok=True)
        
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        try:
            # ä¿å­˜è‹±æ–‡ç‰ˆ
            if 'english' in versions:
                english_file = output_dir / f"daily_news_english_{date_str}.md"
                with open(english_file, 'w', encoding='utf-8') as f:
                    f.write(versions['english'])
                print(f"   ğŸ“„ è‹±æ–‡ç‰ˆ: {english_file}")
            
            # ä¿å­˜ä¸­è‹±æ··åˆç‰ˆ
            if 'bilingual' in versions:
                bilingual_file = output_dir / f"daily_news_bilingual_{date_str}.md"
                with open(bilingual_file, 'w', encoding='utf-8') as f:
                    f.write(versions['bilingual'])
                print(f"   ğŸŒ ä¸­è‹±æ··åˆç‰ˆ: {bilingual_file}")
            
            # ä¿å­˜SRTå­—å¹•ç‰ˆ
            if 'srt' in versions:
                srt_file = output_dir / f"podcast_subtitles_{date_str}.srt"
                with open(srt_file, 'w', encoding='utf-8') as f:
                    f.write(versions['srt'])
                print(f"   ğŸ¬ SRTå­—å¹•ç‰ˆ: {srt_file}")
            
            # ä¿å­˜æœ€æ–°æŠ¥å‘Šï¼ˆç”¨äºGitHub Pagesï¼‰
            latest_file = Path("latest_report.md")
            with open(latest_file, 'w', encoding='utf-8') as f:
                f.write(f"""# ğŸ“° æœ€æ–°ç§‘æŠ€ç®€æŠ¥ - {date_str}

## ğŸ”— ä»Šæ—¥æŠ¥å‘Š

- [ğŸ“„ è‹±æ–‡åŸç‰ˆ](output/daily_news_english_{date_str}.md)
- [ğŸŒ ä¸­è‹±æ··åˆç‰ˆ](output/daily_news_bilingual_{date_str}.md)  
- [ğŸ¬ SRTå­—å¹•æ–‡ä»¶](output/podcast_subtitles_{date_str}.srt)

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

- **æ”¶é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **èµ„è®¯æ€»æ•°**: {len(classified_data)} æ¡
- **ä¿¡æ¯æº**: 60+ ä¸ªä¼˜è´¨ç§‘æŠ€åª’ä½“
- **è¦†ç›–é¢†åŸŸ**: AIã€GitHubã€åˆ›ä¸šã€ç¡…è°·ã€ç§‘æŠ€ã€å¼€æºã€ä¼ä¸šã€ç§»åŠ¨ã€å®‰å…¨ã€Webå¼€å‘

## ğŸ¬ SRTå­—å¹•ä½¿ç”¨è¯´æ˜

SRTå­—å¹•æ–‡ä»¶å¯ç›´æ¥å¯¼å…¥ä»¥ä¸‹è§†é¢‘ç¼–è¾‘è½¯ä»¶ï¼š
- Adobe Premiere Pro
- Final Cut Pro
- DaVinci Resolve
- Camtasia
- OBS Studio

ä½¿ç”¨æ–¹æ³•ï¼š
1. å½•åˆ¶æ’­å®¢éŸ³é¢‘
2. åœ¨è§†é¢‘ç¼–è¾‘è½¯ä»¶ä¸­å¯¼å…¥éŸ³é¢‘å’ŒSRTæ–‡ä»¶
3. è½¯ä»¶ä¼šè‡ªåŠ¨åŒæ­¥å­—å¹•æ—¶é—´è½´
4. æ ¹æ®éœ€è¦è°ƒæ•´å­—å¹•æ ·å¼å’Œä½ç½®

---
*ç”±ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v2.1 è‡ªåŠ¨ç”Ÿæˆ*
""")
            print(f"   ğŸ“‹ æœ€æ–°æŠ¥å‘Š: {latest_file}")
            
            # ä¿å­˜ç»Ÿè®¡æ•°æ®
            stats_file = output_dir / f"stats_{date_str}.json"
            stats_data = {
                'date': date_str,
                'total_articles': len(classified_data),
                'generation_time': datetime.now().isoformat(),
                'versions': {
                    'english_chars': len(versions.get('english', '')),
                    'bilingual_chars': len(versions.get('bilingual', '')),
                    'srt_subtitles': versions.get('srt', '').count('\n\n') if versions.get('srt') else 0
                },
                'categories': {}
            }
            
            # ç»Ÿè®¡åˆ†ç±»
            for item in classified_data:
                category = item.get('category', item.get('ä¸»åˆ†ç±»', 'unknown'))
                stats_data['categories'][category] = stats_data['categories'].get(category, 0) + 1
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ“Š ç»Ÿè®¡æ•°æ®: {stats_file}")
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç”Ÿæˆè¿è¡Œç»Ÿè®¡
        stats = {
            'date': date_str,
            'total_articles': len(classified_data),
            'categories': {},
            'sources': {},
            'timestamp': datetime.now().isoformat()
        }
        
        for news in classified_data:
            # ç»Ÿè®¡åˆ†ç±»
            main_cat = news.get('ä¸»åˆ†ç±»', 'æœªåˆ†ç±»')
            stats['categories'][main_cat] = stats['categories'].get(main_cat, 0) + 1
            
            # ç»Ÿè®¡æ¥æº
            source = news.get('source', 'æœªçŸ¥')
            stats['sources'][source] = stats['sources'].get(source, 0) + 1
        
        stats_file = output_dir / f"stats_{date_str}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ å·²ä¿å­˜: {stats_file}")
    
    def _generate_error_report(self, error_message: str):
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        try:
            error_report = f"""# ğŸš¨ ç³»ç»Ÿé”™è¯¯æŠ¥å‘Š

**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**é”™è¯¯**: {error_message}

## å»ºè®®æ£€æŸ¥é¡¹ç›®
1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. RSSæºæ˜¯å¦å¯è®¿é—®
3. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®é…ç½®
4. GitHub Actionsæƒé™æ˜¯å¦å……è¶³

## æŠ€æœ¯æ”¯æŒ
å¦‚éœ€å¸®åŠ©ï¼Œè¯·åœ¨GitHubä»“åº“æäº¤Issueã€‚
"""
            
            with open('error_report.md', 'w', encoding='utf-8') as f:
                f.write(error_report)
            print("ğŸ“ å·²ç”Ÿæˆé”™è¯¯æŠ¥å‘Š: error_report.md")
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v2.1")
    print("ğŸ¯ ä¸“æ³¨åå¤§æ ¸å¿ƒé¢†åŸŸï¼Œ60+ä¼˜è´¨ä¿¡æ¯æº")
    print("ğŸ¬ æ”¯æŒSRTå­—å¹•æ–‡ä»¶ç”Ÿæˆ")
    print("=" * 60)
    
    try:
        aggregator = GitHubOptimizedAggregator()
        aggregator.run_daily_collection()
        
        print("\nğŸ‰ ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")
        print("ğŸ’¡ æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - output/ ç›®å½•ä¸‹çš„æŠ¥å‘Šæ–‡ä»¶")
        print("   - latest_report.md æœ€æ–°æŠ¥å‘Šç´¢å¼•")
        print("   - SRTå­—å¹•æ–‡ä»¶å¯ç›´æ¥ç”¨äºæ’­å®¢åˆ¶ä½œ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 