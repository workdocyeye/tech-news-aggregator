#!/usr/bin/env python3
"""
ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v2.1 - GitHub Actionsä¼˜åŒ–ç‰ˆ
ä¸“ä¸ºGitHub Actionsç¯å¢ƒä¼˜åŒ–ï¼Œæä¾›æ›´å¥½çš„ç¨³å®šæ€§å’Œé”™è¯¯å¤„ç†
"""

import os
import sys
import time
import json
import feedparser
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import traceback

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from premium_sources import GitHubOptimizedSources
from intelligent_classifier import IntelligentClassifier
from report_generator import ReportGenerator
from notification_system import NotificationSystem

class GitHubOptimizedAggregator:
    """GitHub Actionsä¼˜åŒ–ç‰ˆç§‘æŠ€èµ„è®¯èšåˆå™¨"""
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–GitHub Actionsä¼˜åŒ–ç‰ˆèšåˆå™¨...")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.sources_manager = GitHubOptimizedSources()
        self.classifier = IntelligentClassifier()
        self.report_generator = ReportGenerator()
        self.notification_system = NotificationSystem()
        
        # é…ç½®å‚æ•°
        self.max_retries = 3
        self.retry_delay = 2  # ç§’
        self.timeout = 30  # ç§’
        self.min_articles_threshold = 5  # æœ€å°‘æ–‡ç« æ•°é‡
        
        print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def run_daily_collection(self):
        """è¿è¡Œæ¯æ—¥æ”¶é›†ä»»åŠ¡ - GitHub Actionsä¼˜åŒ–ç‰ˆ"""
        print("\nğŸš€ å¼€å§‹æ¯æ—¥ç§‘æŠ€èµ„è®¯æ”¶é›† (GitHub Actionsä¼˜åŒ–ç‰ˆ)")
        print("ğŸ¯ ä¸“æ³¨äº”å¤§æ ¸å¿ƒé¢†åŸŸï¼šGitHubã€AIã€åˆ›ä¸šã€ç¡…è°·ã€ç§‘æŠ€")
        print("ğŸ›¡ï¸ å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("=" * 60)
        
        try:
            # 1. æ”¶é›†RSSæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            print("ğŸ“¡ æ­£åœ¨æ”¶é›†RSSæ•°æ®...")
            news_data = self._collect_rss_data_with_retry()
            print(f"âœ… æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(news_data)} æ¡èµ„è®¯")
            
            # æ£€æŸ¥æœ€å°‘æ–‡ç« æ•°é‡
            if len(news_data) < self.min_articles_threshold:
                print(f"âš ï¸ æ–‡ç« æ•°é‡è¿‡å°‘ ({len(news_data)} < {self.min_articles_threshold})")
                print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨ç­–ç•¥...")
                news_data = self._fallback_collection_strategy()
            
            # 2. æ™ºèƒ½åˆ†ç±»å’Œå¤„ç†
            print("ğŸ§  æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†ç±»å’Œå¤„ç†...")
            classified_data = self._classify_and_process(news_data)
            print(f"âœ… åˆ†ç±»å®Œæˆï¼Œä¿ç•™ {len(classified_data)} æ¡é«˜è´¨é‡èµ„è®¯")
            
            # 3. ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡
            self._print_classification_stats(classified_data)
            
            # 4. ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬çš„æŠ¥å‘Š
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆå¤šç‰ˆæœ¬æŠ¥å‘Š...")
            versions = self.report_generator.generate_all_versions(classified_data)
            print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            # 5. å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            print("ğŸ“§ æ­£åœ¨å‘é€é‚®ä»¶é€šçŸ¥...")
            try:
                self.notification_system.send_daily_reports(versions)
                print("âœ… é‚®ä»¶å‘é€æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")
                print("ğŸ“§ é‚®ä»¶åŠŸèƒ½å¯èƒ½éœ€è¦é…ç½®ç¯å¢ƒå˜é‡")
            
            # 6. ä¿å­˜åˆ°GitHub
            self._save_for_github(versions, classified_data)
            
            print("ğŸ‰ æ¯æ—¥æ”¶é›†ä»»åŠ¡å®Œæˆï¼")
            print("ğŸ’¡ åŸºäºGitHub Actionsä¼˜åŒ–çš„é«˜è´¨é‡ä¿¡æ¯æº")
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            traceback.print_exc()
            
            # å°è¯•ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            self._generate_error_report(str(e))
            sys.exit(1)
    
    def _collect_rss_data_with_retry(self) -> List[Dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„RSSæ•°æ®æ”¶é›†"""
        all_news = []
        sources = self.sources_manager.get_all_sources()
        
        # æŒ‰å¯é æ€§æ’åºï¼Œä¼˜å…ˆå¤„ç†é«˜å¯é æ€§æº
        sources.sort(key=lambda x: (
            x.get('reliability') == 'excellent',
            x.get('priority') == 'high'
        ), reverse=True)
        
        successful_sources = 0
        failed_sources = []
        
        for source in sources:
            print(f"ğŸ“¡ å¤„ç†: {source['name']}...", end=' ')
            
            success = False
            for attempt in range(self.max_retries):
                try:
                    # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (compatible; TechNewsBot/2.1; +https://github.com/tech-news-bot)'
                    }
                    
                    # ä½¿ç”¨requestsè·å–RSSå†…å®¹
                    response = requests.get(
                        source['url'], 
                        headers=headers, 
                        timeout=self.timeout
                    )
                    response.raise_for_status()
                    
                    # è§£æRSS
                    feed = feedparser.parse(response.content)
                    
                    if not feed.entries:
                        print(f"âŒ æ— å†…å®¹ (å°è¯• {attempt + 1}/{self.max_retries})")
                        if attempt < self.max_retries - 1:
                            time.sleep(self.retry_delay)
                        continue
                    
                    # å¤„ç†RSSæ¡ç›®
                    recent_count = 0
                    for entry in feed.entries[:20]:  # é™åˆ¶å¤„ç†æ•°é‡
                        try:
                            news_item = self._parse_rss_entry(entry, source)
                            if news_item and self._is_recent_news(news_item.get('time')):
                                all_news.append(news_item)
                                recent_count += 1
                        except Exception as parse_error:
                            print(f"è§£ææ¡ç›®é”™è¯¯: {parse_error}")
                            continue
                    
                    print(f"âœ… {recent_count}æ¡")
                    successful_sources += 1
                    success = True
                    break
                    
                except requests.exceptions.Timeout:
                    print(f"â° è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                except requests.exceptions.RequestException as e:
                    print(f"ğŸŒ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:30]}...")
                except Exception as e:
                    print(f"âŒ é”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:30]}...")
                
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
            
            if not success:
                failed_sources.append(source['name'])
        
        print(f"\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸæº: {successful_sources}/{len(sources)}")
        print(f"   âŒ å¤±è´¥æº: {len(failed_sources)}")
        if failed_sources:
            print(f"   å¤±è´¥åˆ—è¡¨: {', '.join(failed_sources[:5])}{'...' if len(failed_sources) > 5 else ''}")
        
        return all_news
    
    def _fallback_collection_strategy(self) -> List[Dict]:
        """å¤‡ç”¨æ”¶é›†ç­–ç•¥"""
        print("ğŸ”„ æ‰§è¡Œå¤‡ç”¨æ”¶é›†ç­–ç•¥...")
        
        # 1. æ‰©å±•æ—¶é—´çª—å£åˆ°48å°æ—¶
        print("   ğŸ“… æ‰©å±•æ—¶é—´çª—å£åˆ°48å°æ—¶")
        all_news = []
        
        # åªä½¿ç”¨æœ€å¯é çš„æº
        reliable_sources = self.sources_manager.get_excellent_reliability_sources()
        
        for source in reliable_sources[:10]:  # é™åˆ¶æ•°é‡
            try:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries[:30]:  # å¢åŠ å¤„ç†æ•°é‡
                    try:
                        news_item = self._parse_rss_entry(entry, source)
                        if news_item:
                            # æ‰©å±•æ—¶é—´çª—å£
                            cutoff_time = datetime.now() - timedelta(hours=48)
                            if news_item.get('time') and news_item['time'] > cutoff_time:
                                all_news.append(news_item)
                    except:
                        continue
            except:
                continue
        
        print(f"   ğŸ“° å¤‡ç”¨ç­–ç•¥æ”¶é›†åˆ° {len(all_news)} æ¡èµ„è®¯")
        return all_news
    
    def _parse_rss_entry(self, entry, source: Dict) -> Optional[Dict]:
        """è§£æRSSæ¡ç›®"""
        try:
            # è·å–åŸºæœ¬ä¿¡æ¯
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            summary = entry.get('summary', entry.get('description', '')).strip()
            link = entry.get('link', '')
            
            # è§£ææ—¶é—´
            pub_time = self._parse_time(entry.get('published', ''))
            
            return {
                'title': title,
                'summary': summary[:500],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                'url': link,
                'source': source['name'],
                'category': source['category'],
                'time': pub_time,
                'priority': source.get('priority', 'medium'),
                'reliability': source.get('reliability', 'good')
            }
        except Exception as e:
            print(f"è§£æRSSæ¡ç›®é”™è¯¯: {e}")
            return None
    
    def _parse_time(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            import dateutil.parser
            parsed_time = dateutil.parser.parse(time_str)
            if parsed_time.tzinfo is not None:
                parsed_time = parsed_time.replace(tzinfo=None)
            return parsed_time
        except:
            return datetime.now()
    
    def _is_recent_news(self, news_time) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ€è¿‘çš„æ–°é—»"""
        if not isinstance(news_time, datetime):
            return True
        
        cutoff_time = datetime.now() - timedelta(hours=24)
        return news_time > cutoff_time
    
    def _classify_and_process(self, news_data: List[Dict]) -> List[Dict]:
        """åˆ†ç±»å’Œå¤„ç†æ–°é—»æ•°æ®"""
        if not news_data:
            return []
        
        # å»é‡
        unique_news = self._remove_duplicates(news_data)
        print(f"   ğŸ”„ å»é‡å: {len(unique_news)} æ¡")
        
        # æ™ºèƒ½åˆ†ç±»
        classified_news = []
        for news in unique_news:
            try:
                classified = self.classifier.classify_news(news)
                classified_news.append(classified)
            except Exception as e:
                print(f"åˆ†ç±»é”™è¯¯: {e}")
                classified_news.append(news)  # ä¿ç•™åŸå§‹æ•°æ®
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        classified_news.sort(
            key=lambda x: x.get('ç»¼åˆè¯„åˆ†', 0), 
            reverse=True
        )
        
        return classified_news
    
    def _remove_duplicates(self, news_data: List[Dict]) -> List[Dict]:
        """å»é‡å¤„ç†"""
        seen_titles = set()
        unique_news = []
        
        for news in news_data:
            title_key = news.get('title', '').lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news
    
    def _print_classification_stats(self, classified_data: List[Dict]):
        """æ‰“å°åˆ†ç±»ç»Ÿè®¡"""
        if not classified_data:
            print("âš ï¸ æ²¡æœ‰åˆ†ç±»æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†ç±»
        category_stats = {}
        for news in classified_data:
            main_cat = news.get('ä¸»åˆ†ç±»', 'æœªåˆ†ç±»')
            category_stats[main_cat] = category_stats.get(main_cat, 0) + 1
        
        print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"   ğŸ“‚ {category}: {count}æ¡")
    
    def _save_for_github(self, versions: Dict[str, str], classified_data: List[Dict]):
        """ä¿å­˜æ–‡ä»¶ä¾›GitHub Actionsä½¿ç”¨"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path('output')
        output_dir.mkdir(exist_ok=True)
        
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # ä¿å­˜å„ç‰ˆæœ¬æŠ¥å‘Š
        for version_name, content in versions.items():
            file_path = output_dir / f"{version_name}_{date_str}.md"
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"   ğŸ’¾ å·²ä¿å­˜: {file_path}")
        
        # ä¿å­˜åˆ†ç±»æ•°æ®
        classified_file = output_dir / f"classified_data_{date_str}.json"
        with open(classified_file, 'w', encoding='utf-8') as f:
            json.dump(classified_data, f, ensure_ascii=False, indent=2, default=str)
        print(f"   ğŸ’¾ å·²ä¿å­˜: {classified_file}")
        
        # ä¿å­˜æœ€æ–°æŠ¥å‘Šï¼ˆç”¨äºç½‘é¡µå±•ç¤ºï¼‰
        with open('latest_report.md', 'w', encoding='utf-8') as f:
            f.write(versions.get('bilingual', versions.get('english', '')))
        print(f"   ğŸ’¾ å·²ä¿å­˜: latest_report.md")
        
        # ç”Ÿæˆè¿è¡Œç»Ÿè®¡
        stats = {
            'date': date_str,
            'total_articles': len(classified_data),
            'categories': {},
            'sources': {},
            'timestamp': datetime.now().isoformat()
        }
        
        for news in classified_data:
            # ç»Ÿè®¡åˆ†ç±»
            main_cat = news.get('ä¸»åˆ†ç±»', 'æœªåˆ†ç±»')
            stats['categories'][main_cat] = stats['categories'].get(main_cat, 0) + 1
            
            # ç»Ÿè®¡æ¥æº
            source = news.get('source', 'æœªçŸ¥')
            stats['sources'][source] = stats['sources'].get(source, 0) + 1
        
        stats_file = output_dir / f"stats_{date_str}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ å·²ä¿å­˜: {stats_file}")
    
    def _generate_error_report(self, error_message: str):
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        try:
            error_report = f"""# ğŸš¨ ç³»ç»Ÿé”™è¯¯æŠ¥å‘Š

**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**é”™è¯¯**: {error_message}

## å»ºè®®æ£€æŸ¥é¡¹ç›®
1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. RSSæºæ˜¯å¦å¯è®¿é—®
3. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®é…ç½®
4. GitHub Actionsæƒé™æ˜¯å¦å……è¶³

## æŠ€æœ¯æ”¯æŒ
å¦‚éœ€å¸®åŠ©ï¼Œè¯·åœ¨GitHubä»“åº“æäº¤Issueã€‚
"""
            
            with open('error_report.md', 'w', encoding='utf-8') as f:
                f.write(error_report)
            print("ğŸ“ å·²ç”Ÿæˆé”™è¯¯æŠ¥å‘Š: error_report.md")
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v2.1 - GitHub Actionsä¼˜åŒ–ç‰ˆ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    if os.getenv('GITHUB_ACTIONS'):
        print("âœ… è¿è¡Œåœ¨GitHub Actionsç¯å¢ƒ")
    else:
        print("ğŸ’» è¿è¡Œåœ¨æœ¬åœ°ç¯å¢ƒ")
    
    # åˆ›å»ºèšåˆå™¨å¹¶è¿è¡Œ
    aggregator = GitHubOptimizedAggregator()
    aggregator.run_daily_collection()

if __name__ == "__main__":
    main() 