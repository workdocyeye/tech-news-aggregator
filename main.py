#!/usr/bin/env python3
"""
ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v3.0
ç®€åŒ–ç‰ˆæœ¬ - åªç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ï¼šä¸­æ–‡ç‰ˆæœ¬ã€è‹±æ–‡ç‰ˆæœ¬ã€SRTä¸­æ–‡å­—å¹•
"""

import feedparser
import requests
from datetime import datetime, timedelta
from pathlib import Path
import json
import sys
import traceback
from typing import List, Dict, Optional
import time
import re
from bs4 import BeautifulSoup

# å¯¼å…¥æ¨¡å—
from rss_sources import get_rss_sources
from smart_translator import SmartTranslator
from srt_generator import SRTGenerator

class TechNewsAggregator:
    """ç§‘æŠ€èµ„è®¯èšåˆå™¨ - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.sources = get_rss_sources()
        
        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        try:
            self.translator = SmartTranslator()
            self.translation_enabled = True
            print("âœ… æ™ºèƒ½ç¿»è¯‘å™¨å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.translator = None
            self.translation_enabled = False
        
        # åˆå§‹åŒ–SRTç”Ÿæˆå™¨
        self.srt_generator = SRTGenerator()
        print("âœ… SRTå­—å¹•ç”Ÿæˆå™¨å·²å¯ç”¨")
    
    def run_daily_collection(self):
        """è¿è¡Œæ¯æ—¥æ”¶é›†ä»»åŠ¡"""
        print(f"ğŸš€ å¼€å§‹æ”¶é›†ç§‘æŠ€èµ„è®¯...")
        print(f"ğŸ“¡ RSSæºæ•°é‡: {len(self.sources)}")
        
        # æ”¶é›†RSSæ•°æ®
        news_data = self._collect_rss_data()
        
        if not news_data:
            print("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            self._generate_error_report("æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return
        
        print(f"ğŸ“° æ”¶é›†åˆ° {len(news_data)} æ¡æ–°é—»")
        
        # å»é‡å’Œè¿‡æ»¤
        filtered_data = self._remove_duplicates(news_data)
        print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(filtered_data)} æ¡æ–°é—»")
        
        # ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬çš„æŠ¥å‘Š
        self._generate_reports(filtered_data)
        
        print("âœ… ä»»åŠ¡å®Œæˆï¼")
    
    def _collect_rss_data(self) -> List[Dict]:
        """æ”¶é›†RSSæ•°æ®"""
        all_news = []
        
        for source in self.sources:
            try:
                print(f"ğŸ“¡ æ­£åœ¨æ”¶é›†: {source['name']}")
                
                # è®¾ç½®è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                # è§£æRSS
                feed = feedparser.parse(source['url'], request_headers=headers)
                
                if not feed.entries:
                    print(f"   âš ï¸ {source['name']}: æ— æ•°æ®")
                    continue
                
                # å¤„ç†æ¯ä¸ªæ¡ç›®
                for entry in feed.entries:
                    news_item = self._parse_rss_entry(entry, source)
                    if news_item and self._is_recent_news(news_item.get('published_time')):
                        all_news.append(news_item)
                
                print(f"   âœ… {source['name']}: æ”¶é›†åˆ° {len([e for e in feed.entries if self._parse_rss_entry(e, source)])} æ¡")
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                print(f"   âŒ {source['name']}: æ”¶é›†å¤±è´¥ - {e}")
                continue
        
        return all_news
    
    def _parse_rss_entry(self, entry, source: Dict) -> Optional[Dict]:
        """è§£æRSSæ¡ç›®"""
        try:
            # è·å–æ ‡é¢˜
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            # è·å–æ‘˜è¦
            summary = ''
            if hasattr(entry, 'summary'):
                summary = entry.summary
            elif hasattr(entry, 'description'):
                summary = entry.description
            
            # æ¸…ç†HTMLæ ‡ç­¾
            if summary:
                summary = BeautifulSoup(summary, 'html.parser').get_text().strip()
            
            # è·å–é“¾æ¥
            link = entry.get('link', '')
            
            # è·å–å‘å¸ƒæ—¶é—´
            published_time = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_time = datetime(*entry.published_parsed[:6])
            
            return {
                'title': title,
                'summary': summary[:300] if summary else '',  # é™åˆ¶æ‘˜è¦é•¿åº¦
                'url': link,
                'source': source['name'],
                'category': source['category'],
                'published_time': published_time,
                'time': published_time.strftime('%Y-%m-%d %H:%M') if published_time else ''
            }
            
        except Exception as e:
            return None
    
    def _is_recent_news(self, news_time) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ€è¿‘çš„æ–°é—»ï¼ˆ24å°æ—¶å†…ï¼‰"""
        if not news_time:
            return True  # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«
        
        try:
            if isinstance(news_time, str):
                return True  # ç®€åŒ–å¤„ç†
            
            now = datetime.now()
            time_diff = now - news_time
            return time_diff.days <= 1
        except:
            return True
    
    def _remove_duplicates(self, news_data: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤æ–°é—»"""
        seen_titles = set()
        unique_news = []
        
        for news in news_data:
            title = news.get('title', '').lower().strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        # æŒ‰æ—¶é—´æ’åº
        unique_news.sort(key=lambda x: x.get('published_time') or datetime.min, reverse=True)
        
        return unique_news  # ä¿ç•™å…¨éƒ¨æ–°é—»ï¼Œä¸åšæ•°é‡é™åˆ¶
    
    def _generate_reports(self, news_data: List[Dict]):
        """ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬çš„æŠ¥å‘Š"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path('output')
        output_dir.mkdir(exist_ok=True)
        
        print("ğŸ“ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        
        # 1. ç”Ÿæˆè‹±æ–‡ç‰ˆ
        english_content = self._generate_english_report(news_data)
        english_file = output_dir / f"tech_news_english_{date_str}.md"
        with open(english_file, 'w', encoding='utf-8') as f:
            f.write(english_content)
        print(f"   ğŸ“„ è‹±æ–‡ç‰ˆ: {english_file}")
        
        # 2. ç”Ÿæˆä¸­æ–‡ç‰ˆ
        chinese_content = self._generate_chinese_report(news_data)
        chinese_file = output_dir / f"tech_news_chinese_{date_str}.md"
        with open(chinese_file, 'w', encoding='utf-8') as f:
            f.write(chinese_content)
        print(f"   ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ: {chinese_file}")
        
        # 3. ç”ŸæˆSRTå­—å¹•
        srt_content = self.srt_generator.generate_srt_from_news(news_data)
        srt_file = output_dir / f"tech_news_subtitles_{date_str}.srt"
        with open(srt_file, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"   ğŸ¬ SRTå­—å¹•: {srt_file}")
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        self._generate_index_file(date_str, len(news_data))
        
        print("âœ… æ‰€æœ‰æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def _generate_english_report(self, news_data: List[Dict]) -> str:
        """ç”Ÿæˆè‹±æ–‡ç‰ˆæŠ¥å‘Š"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        content = f"""# ğŸŒ Daily Tech News - {date_str}

## ğŸ“Š Today's Overview
- **Total Articles**: {len(news_data)}
- **Sources**: {len(set(item['source'] for item in news_data))} tech media outlets
- **Last Updated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ”¥ Top Stories

"""
        
        # æ·»åŠ å‰20æ¡é‡ç‚¹æ–°é—»
        for i, item in enumerate(news_data[:20], 1):
            title = item.get('title', 'No Title')
            summary = item.get('summary', 'No summary available')
            source = item.get('source', 'Unknown')
            time_str = item.get('time', 'Unknown')
            url = item.get('url', '#')
            
            content += f"""### {i}. {title}
**Source**: {source} | **Time**: {time_str}
**Summary**: {summary}
**Link**: [Read More]({url})

"""
        
        # æŒ‰åˆ†ç±»æ·»åŠ å…¶ä»–æ–°é—»
        categories = {}
        for item in news_data[20:]:
            category = item.get('category', 'Other')
            if category not in categories:
                categories[category] = []
            categories[category].append(item)
        
        for category, items in categories.items():
            if items:
                content += f"\n## ğŸ’» {category}\n\n"
                for item in items[:15]:  # æ¯ä¸ªåˆ†ç±»å¢åŠ åˆ°15æ¡
                    title = item.get('title', 'No Title')
                    url = item.get('url', '#')
                    source = item.get('source', 'Unknown')
                    
                    content += f"- **[{title}]({url})** - *{source}*\n"
        
        content += f"""
---
*Generated on {date_str} by Tech News Aggregator v3.0*
*Total sources processed: {len(news_data)} articles*
"""
        
        return content
    
    def _generate_chinese_report(self, news_data: List[Dict]) -> str:
        """ç”Ÿæˆä¸­æ–‡ç‰ˆæŠ¥å‘Š"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        content = f"""# ğŸŒŸ æ¯æ—¥ç§‘æŠ€ç®€æŠ¥ - {date_str}

## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ
- **èµ„è®¯æ€»æ•°**: {len(news_data)} æ¡
- **ä¿¡æ¯æº**: {len(set(item['source'] for item in news_data))} ä¸ªç§‘æŠ€åª’ä½“
- **æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ”¥ é‡ç‚¹æ–°é—»

"""
        
        # æ·»åŠ å‰20æ¡é‡ç‚¹æ–°é—»ï¼ˆç¿»è¯‘ç‰ˆæœ¬ï¼‰
        for i, item in enumerate(news_data[:20], 1):
            title = item.get('title', 'No Title')
            summary = item.get('summary', 'No summary available')
            source = item.get('source', 'Unknown')
            time_str = item.get('time', 'Unknown')
            url = item.get('url', '#')
            
            # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦
            if self.translation_enabled:
                try:
                    title_cn = self.translator.translate_content(title)
                    summary_cn = self.translator.translate_content(summary) if summary else 'æš‚æ— æ‘˜è¦'
                except:
                    title_cn = title
                    summary_cn = summary
            else:
                title_cn = title
                summary_cn = summary
            
            content += f"""### {i}. {title_cn}
**æ¥æº**: {source} | **æ—¶é—´**: {time_str}
**æ‘˜è¦**: {summary_cn}
**åŸæ–‡**: [æŸ¥çœ‹è¯¦æƒ…]({url})

"""
        
        # æŒ‰åˆ†ç±»æ·»åŠ å…¶ä»–æ–°é—»
        categories = {}
        for item in news_data[20:]:
            category = item.get('category', 'Other')
            if category not in categories:
                categories[category] = []
            categories[category].append(item)
        
        # åˆ†ç±»åç§°æ˜ å°„
        category_map = {
            'Tech News': 'ç§‘æŠ€æ–°é—»',
            'Tech Research': 'æŠ€æœ¯ç ”ç©¶',
            'Open Source': 'å¼€æºé¡¹ç›®',
            'Tech Community': 'æŠ€æœ¯ç¤¾åŒº',
            'Startups': 'åˆ›ä¸šèµ„è®¯',
            'Enterprise Tech': 'ä¼ä¸šæŠ€æœ¯',
            'Mobile': 'ç§»åŠ¨åº”ç”¨',
            'Security': 'ç½‘ç»œå®‰å…¨',
            'Web Dev': 'Webå¼€å‘',
            'Cloud': 'äº‘è®¡ç®—',
            'Data Science': 'æ•°æ®ç§‘å­¦',
            'Gaming Tech': 'æ¸¸æˆæŠ€æœ¯',
            'AI/ML': 'AI/æœºå™¨å­¦ä¹ ',
            'Silicon Valley': 'ç¡…è°·åŠ¨æ€'
        }
        
        for category, items in categories.items():
            if items:
                category_cn = category_map.get(category, category)
                content += f"\n## ğŸ’» {category_cn}\n\n"
                for item in items[:15]:  # æ¯ä¸ªåˆ†ç±»å¢åŠ åˆ°15æ¡
                    title = item.get('title', 'No Title')
                    url = item.get('url', '#')
                    source = item.get('source', 'Unknown')
                    
                    # ç¿»è¯‘æ ‡é¢˜
                    if self.translation_enabled:
                        try:
                            title_cn = self.translator.translate_content(title)
                        except:
                            title_cn = title
                    else:
                        title_cn = title
                    
                    content += f"- **[{title_cn}]({url})** - *{source}*\n"
        
        content += f"""
---
*ç”±ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v3.0 äº {date_str} è‡ªåŠ¨ç”Ÿæˆ*
*å…±å¤„ç† {len(news_data)} æ¡èµ„è®¯*
"""
        
        return content
    
    def _generate_index_file(self, date_str: str, total_count: int):
        """ç”Ÿæˆç´¢å¼•æ–‡ä»¶"""
        index_content = f"""# ğŸ“° ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ

## ğŸ”— ä»Šæ—¥æŠ¥å‘Š - {date_str}

- [ğŸ“„ è‹±æ–‡ç‰ˆ](output/tech_news_english_{date_str}.md)
- [ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ](output/tech_news_chinese_{date_str}.md)
- [ğŸ¬ SRTå­—å¹•](output/tech_news_subtitles_{date_str}.srt)

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **èµ„è®¯æ€»æ•°**: {total_count} æ¡
- **ä¿¡æ¯æº**: {len(self.sources)} ä¸ªä¼˜è´¨ç§‘æŠ€åª’ä½“

## ğŸ¬ SRTå­—å¹•ä½¿ç”¨è¯´æ˜

SRTå­—å¹•æ–‡ä»¶å¯ç›´æ¥ç”¨äºè§†é¢‘ç¼–è¾‘è½¯ä»¶ï¼š
- Adobe Premiere Pro
- Final Cut Pro
- DaVinci Resolve
- Camtasia
- OBS Studio

---
*ç”±ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v3.0 è‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(index_content)
        print(f"   ğŸ“‹ ç´¢å¼•æ–‡ä»¶: README.md")
    
    def _generate_error_report(self, error_message: str):
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        try:
            error_report = f"""# ğŸš¨ ç³»ç»Ÿé”™è¯¯æŠ¥å‘Š

**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**é”™è¯¯**: {error_message}

## å»ºè®®æ£€æŸ¥é¡¹ç›®
1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. RSSæºæ˜¯å¦å¯è®¿é—®
3. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®é…ç½®

---
*ç”±ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v3.0 ç”Ÿæˆ*
"""
            
            with open('error_report.md', 'w', encoding='utf-8') as f:
                f.write(error_report)
            print("ğŸ“ å·²ç”Ÿæˆé”™è¯¯æŠ¥å‘Š: error_report.md")
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ç§‘æŠ€èµ„è®¯èšåˆç³»ç»Ÿ v3.0")
    print("ğŸ¯ ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬ï¼šä¸­æ–‡ç‰ˆã€è‹±æ–‡ç‰ˆã€SRTå­—å¹•")
    print("=" * 50)
    
    try:
        aggregator = TechNewsAggregator()
        aggregator.run_daily_collection()
        
        print("\nğŸ‰ ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")
        print("ğŸ’¡ æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - output/ ç›®å½•ä¸‹çš„ä¸‰ä¸ªæ–‡ä»¶")
        print("   - README.md ç´¢å¼•æ–‡ä»¶")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 